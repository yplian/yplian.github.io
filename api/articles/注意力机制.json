{"title":"注意力机制相关","slug":"注意力机制","date":"2025-05-24T04:00:00.000Z","updated":"2025-05-24T03:00:00.000Z","comments":true,"path":"api/articles/注意力机制.json","excerpt":"注意力机制简单了解。<br>","covers":null,"content":"<p>注意力机制简单了解。<br><a id=\"more\"></a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><h3 id=\"注意力机制的通俗理解\"><a href=\"#注意力机制的通俗理解\" class=\"headerlink\" title=\"注意力机制的通俗理解\"></a>注意力机制的通俗理解</h3><p>让我们通过几个生活中的例子来理解:</p>\n<p>例子1: 阅读理解<br>假设有这样一段话:<br>“小明很喜欢打篮球。他每天都会去操场练习。”</p>\n<p>当我们读到”他”这个字的时候,大脑会自动去寻找这个”他”指的是谁。通过上下文,我们知道”他”指的是”小明”。这就是一个最基本的注意力机制 - 我们的大脑会自动关注到相关的重要信息。</p>\n<p>例子2: 歧义词理解<br>“我在得物上买了最新款的苹果,体验非常好”<br>“我在得物上买了阿克苏的苹果,口感非常好”</p>\n<p>在这两句话中,”苹果”是一个歧义词:</p>\n<ul>\n<li>第一句中的”苹果”指的是iPhone手机,因为搭配了”最新款”和”体验”</li>\n<li>第二句中的”苹果”指的是水果,因为搭配了”阿克苏”和”口感”</li>\n</ul>\n<p>注意力机制就是帮助模型像人类大脑一样,通过上下文来理解每个词的真实含义。</p>\n<h3 id=\"注意力机制是如何工作的\"><a href=\"#注意力机制是如何工作的\" class=\"headerlink\" title=\"注意力机制是如何工作的\"></a>注意力机制是如何工作的</h3><p>让我用一个形象的例子来解释:</p>\n<p>想象你在看一个很长的句子:”昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>当模型在处理”她”这个词时,会:</p>\n<p>1) 计算相关度分数:</p>\n<ul>\n<li>“她” 和 “小明” 的相关度: 较低(因为性别不匹配)</li>\n<li>“她” 和 “小红” 的相关度: 很高(因为性别匹配)</li>\n<li>“她” 和其他词的相关度: 较低</li>\n</ul>\n<p>2) 分配注意力权重:</p>\n<ul>\n<li>“小红” 获得最高的注意力权重</li>\n<li>其他词获得较低的注意力权重</li>\n</ul>\n<p>3) 整合信息:<br>模型会重点关注高权重的信息,从而理解”她”指代的是”小红”。</p>\n<h3 id=\"多头注意力机制\"><a href=\"#多头注意力机制\" class=\"headerlink\" title=\"多头注意力机制\"></a>多头注意力机制</h3><p>继续用刚才的例子:<br>“昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>多头注意力就像是从多个角度来理解这句话:</p>\n<ul>\n<li><p>第1个注意力头可能关注人物关系:<br>发现”她”和”小红”的关联</p>\n</li>\n<li><p>第2个注意力头可能关注时间信息:<br>关注”昨天下午”这个时间点</p>\n</li>\n<li><p>第3个注意力头可能关注地点信息:<br>关注”公园”这个场所</p>\n</li>\n<li><p>第4个注意力头可能关注动作信息:<br>关注”遇到”这个动作</p>\n</li>\n</ul>\n<p>这就像多个人从不同角度来理解同一句话,最后把这些理解综合起来,得到更全面的理解。</p>\n<h3 id=\"为什么需要注意力机制\"><a href=\"#为什么需要注意力机制\" class=\"headerlink\" title=\"为什么需要注意力机制\"></a>为什么需要注意力机制</h3><p>在注意力机制出现之前,模型处理长文本有这些问题:</p>\n<ul>\n<li><p>无法很好地处理长距离依赖<br>比如在很长的文章中,可能会”忘记”前面提到的重要信息</p>\n</li>\n<li><p>不能并行处理<br>必须一个词一个词按顺序处理,效率较低</p>\n</li>\n</ul>\n<p>注意力机制解决了这些问题:</p>\n<ul>\n<li>可以直接建立任意两个词之间的联系</li>\n<li>支持并行计算,大大提高了处理效率</li>\n</ul>\n<h2 id=\"为什么可以并行\"><a href=\"#为什么可以并行\" class=\"headerlink\" title=\"为什么可以并行\"></a>为什么可以并行</h2><p>对比传统模型(RNN)和Transformer来解释为什么Transformer支持并行计算。</p>\n<h3 id=\"传统RNN模型的处理方式\"><a href=\"#传统RNN模型的处理方式\" class=\"headerlink\" title=\"传统RNN模型的处理方式\"></a>传统RNN模型的处理方式</h3><p>假设我们有一句话：”我喜欢吃苹果”</p>\n<p>RNN必须按顺序一个词一个词处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">第1步：处理&quot;我&quot;</span><br><span class=\"line\">第2步：处理&quot;喜欢&quot; (必须等第1步完成)</span><br><span class=\"line\">第3步：处理&quot;吃&quot; (必须等第2步完成)</span><br><span class=\"line\">第4步：处理&quot;苹果&quot; (必须等第3步完成)</span><br></pre></td></tr></table></figure>\n<p>就像是在读一本书，你必须从第一页开始，按顺序一页一页读下去，不能跳着读。这就是”顺序处理”。</p>\n<h3 id=\"Transformer的并行处理方式\"><a href=\"#Transformer的并行处理方式\" class=\"headerlink\" title=\"Transformer的并行处理方式\"></a>Transformer的并行处理方式</h3><p>Transformer可以同时处理所有的词：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">同时处理：</span><br><span class=\"line\">- &quot;我&quot; </span><br><span class=\"line\">- &quot;喜欢&quot;</span><br><span class=\"line\">- &quot;吃&quot;</span><br><span class=\"line\">- &quot;苹果&quot;</span><br></pre></td></tr></table></figure>\n<p>这就像是有4个人，每个人负责读一个词，他们可以同时开始读。这就是”并行处理”。</p>\n<h3 id=\"为什么Transformer能做到并行？\"><a href=\"#为什么Transformer能做到并行？\" class=\"headerlink\" title=\"为什么Transformer能做到并行？\"></a>为什么Transformer能做到并行？</h3><p>关键在于位置编码(Positional Encoding)和自注意力机制：</p>\n<p>a) 位置编码：</p>\n<ul>\n<li>每个词都会被赋予一个位置信息</li>\n<li>比如：”我”(位置1)、”喜欢”(位置2)、”吃”(位置3)、”苹果”(位置4)</li>\n<li>这样即使同时处理所有词，模型也知道每个词在句子中的位置</li>\n</ul>\n<p>举个例子：<br>就像给每个人发一张带编号的扑克牌：</p>\n<ul>\n<li>“我” → 1号牌</li>\n<li>“喜欢” → 2号牌</li>\n<li>“吃” → 3号牌</li>\n<li>“苹果” → 4号牌</li>\n</ul>\n<p>这样即使大家同时看自己的牌，也知道这些词的正确顺序。</p>\n<p>b) 自注意力机制：</p>\n<ul>\n<li>每个词可以同时和句子中的所有其他词计算注意力分数</li>\n<li>不需要等待前面的词处理完成</li>\n</ul>\n<p>举个例子：<br>想象一个会议室里有4个人，每个人负责一个词：</p>\n<ul>\n<li>A负责”我”</li>\n<li>B负责”喜欢”</li>\n<li>C负责”吃”</li>\n<li>D负责”苹果”</li>\n</ul>\n<p>他们可以同时开始工作：</p>\n<ol>\n<li>每个人都有一份完整的句子副本</li>\n<li>他们可以同时查看整个句子</li>\n<li><p>各自计算自己负责的词与其他词的关联度</p>\n</li>\n<li><p>实际的计算优势：</p>\n</li>\n</ol>\n<p>假设处理一个有100个词的句子：</p>\n<ul>\n<li>RNN需要100个时间步骤，因为必须一个词一个词处理</li>\n<li>Transformer只需要1个时间步骤，因为可以同时处理100个词</li>\n</ul>\n<p>这就像是：</p>\n<ul>\n<li>RNN：一个人要读100页书，需要花100分钟</li>\n<li>Transformer：100个人同时读，每人读1页，只需要1分钟</li>\n</ul>\n<h3 id=\"在GPU上的优势\"><a href=\"#在GPU上的优势\" class=\"headerlink\" title=\"在GPU上的优势\"></a>在GPU上的优势</h3><p>现代GPU擅长并行计算，就像有很多个小工人可以同时工作。</p>\n<ul>\n<li>Transformer充分利用了这一优势，可以让GPU同时处理多个词</li>\n<li>而RNN因为必须按顺序处理，无法充分利用GPU的并行能力</li>\n</ul>\n<p>这就是为什么说Transformer支持并行计算，它通过巧妙的设计（位置编码+自注意力机制），让所有词可以同时被处理，大大提高了计算效率。这也是Transformer相比RNN的一个重要优势。</p>\n<h2 id=\"注意力机制计算过程\"><a href=\"#注意力机制计算过程\" class=\"headerlink\" title=\"注意力机制计算过程\"></a>注意力机制计算过程</h2><h3 id=\"基本概念：Query-查询-、Key-键-、Value-值\"><a href=\"#基本概念：Query-查询-、Key-键-、Value-值\" class=\"headerlink\" title=\"基本概念：Query(查询)、Key(键)、Value(值)\"></a>基本概念：Query(查询)、Key(键)、Value(值)</h3><p>先用一个生活中的例子来理解这三个概念：<br>想象你去图书馆找书：</p>\n<ul>\n<li>Query(查询)：你想找的书的关键词，比如”人工智能入门”</li>\n<li>Key(键)：图书馆里每本书的标题</li>\n<li>Value(值)：书的具体内容</li>\n</ul>\n<h3 id=\"在Transformer中的对应关系\"><a href=\"#在Transformer中的对应关系\" class=\"headerlink\" title=\"在Transformer中的对应关系\"></a>在Transformer中的对应关系</h3><p>还是用前面的例子：<br>“昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>当我们要理解”她”这个词时：</p>\n<ul>\n<li>Query(查询)：是”她”这个词的表示向量</li>\n<li>Key(键)：句子中所有词的表示向量</li>\n<li>Value(值)：句子中所有词携带的信息</li>\n</ul>\n<h3 id=\"具体计算步骤\"><a href=\"#具体计算步骤\" class=\"headerlink\" title=\"具体计算步骤\"></a>具体计算步骤</h3><p>让我们用一个简化的例子来说明：<br>“小红很喜欢吃苹果”</p>\n<p>第一步：转换为向量<br>假设我们用2维向量来表示每个词（实际中通常是512维或更高）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;小红&quot; → [2, 3]</span><br><span class=\"line\">&quot;很&quot; → [1, 1]</span><br><span class=\"line\">&quot;喜欢&quot; → [2, 2]</span><br><span class=\"line\">&quot;吃&quot; → [1, 2]</span><br><span class=\"line\">&quot;苹果&quot; → [3, 1]</span><br></pre></td></tr></table></figure>\n<p>第二步：计算Query、Key、Value<br>每个词都会被转换为三种向量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">例如&quot;小红&quot;：</span><br><span class=\"line\">Q向量(查询)：[2, 3] × WQ = [4, 6]</span><br><span class=\"line\">K向量(键)：[2, 3] × WK = [5, 7]</span><br><span class=\"line\">V向量(值)：[2, 3] × WV = [3, 4]</span><br></pre></td></tr></table></figure>\n<p>(其中WQ、WK、WV是可学习的权重矩阵)</p>\n<p>第三步：计算注意力分数<br>以”小红”为例，计算它与所有词的相关度：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">相关度 = Q向量 · K向量 / √维度</span><br><span class=\"line\"></span><br><span class=\"line\">比如&quot;小红&quot;和&quot;苹果&quot;的相关度：</span><br><span class=\"line\">= [4, 6] · [5, 7] / √2</span><br><span class=\"line\">= (4×5 + 6×7) / 1.414</span><br><span class=\"line\">= (20 + 42) / 1.414</span><br><span class=\"line\">= 43.7</span><br></pre></td></tr></table></figure>\n<p>第四步：归一化分数<br>使用softmax函数将所有分数转换为0-1之间的概率值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原始分数：[43.7, 35.2, 28.9, 31.5, 39.6]</span><br><span class=\"line\">归一化后：[0.3, 0.2, 0.1, 0.15, 0.25]</span><br></pre></td></tr></table></figure>\n<p>第五步：加权求和<br>用这些概率值去加权每个词的Value向量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">最终表示 = 0.3×V1 + 0.2×V2 + 0.1×V3 + 0.15×V4 + 0.25×V5</span><br></pre></td></tr></table></figure>\n<h3 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h3><p>实际上，Transformer使用了多头注意力机制，相当于：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- 注意力头1：可能关注语法关系</span><br><span class=\"line\">- 注意力头2：可能关注语义关系</span><br><span class=\"line\">- 注意力头3：可能关注位置关系</span><br><span class=\"line\">...（通常有8个头）</span><br></pre></td></tr></table></figure>\n<p>每个头都独立计算注意力分数，最后将所有结果合并。</p>\n<h3 id=\"形象的比喻\"><a href=\"#形象的比喻\" class=\"headerlink\" title=\"形象的比喻\"></a>形象的比喻</h3><p>这个过程就像：</p>\n<ul>\n<li>一个人(Query)在派对上(句子)寻找聊天对象</li>\n<li>他会看看每个人(Key)是否适合聊天</li>\n<li>计算相关度就像评估与每个人的共同话题多少</li>\n<li>最后主要和相关度高的人(Value)交流</li>\n</ul>\n<p>或者像：</p>\n<ul>\n<li>一个学生(Query)在解答问题</li>\n<li>他会查看所有笔记(Key)</li>\n<li>计算每页笔记与问题的相关度</li>\n<li>最后主要参考相关度高的笔记内容(Value)</li>\n</ul>\n<p>这就是注意力分数的计算过程。虽然数学计算看起来复杂，但基本思想是让模型能够像人类一样，通过计算相关度来决定应该重点关注哪些信息。</p>\n<h2 id=\"向量\"><a href=\"#向量\" class=\"headerlink\" title=\"向量\"></a>向量</h2><h3 id=\"词向量的基本概念\"><a href=\"#词向量的基本概念\" class=\"headerlink\" title=\"词向量的基本概念\"></a>词向量的基本概念</h3><p>词向量(Word Embedding)是将单词转换为计算机可以理解的数值向量的方法。主要有以下特点：</p>\n<ul>\n<li>每个单词被映射到一个固定维度(比如300维)的向量空间中</li>\n<li>语义相近的词在向量空间中的距离也相近</li>\n<li>向量之间可以进行数学运算,比如:<br>vector(‘国王’) - vector(‘男人’) + vector(‘女人’) ≈ vector(‘女王’)</li>\n</ul>\n<h3 id=\"获取词向量的主要方法\"><a href=\"#获取词向量的主要方法\" class=\"headerlink\" title=\"获取词向量的主要方法\"></a>获取词向量的主要方法</h3><p>(1) Word2Vec模型:</p>\n<ul>\n<li>CBOW(Continuous Bag of Words):使用上下文预测目标词</li>\n<li>Skip-gram:使用目标词预测上下文词</li>\n<li>通过神经网络训练,学习词与词之间的语义关系</li>\n</ul>\n<p>(2) GloVe模型:</p>\n<ul>\n<li>基于全局矩阵分解</li>\n<li>统计词与词的共现频率</li>\n<li>结合局部上下文窗口方法</li>\n</ul>\n<p>(3) FastText模型:</p>\n<ul>\n<li>将单词分解为子词单元(n-gram)</li>\n<li>能处理词表外的词</li>\n<li>对拼写错误有一定容忍度</li>\n</ul>\n<h3 id=\"Transformer中的向量表示\"><a href=\"#Transformer中的向量表示\" class=\"headerlink\" title=\"Transformer中的向量表示\"></a>Transformer中的向量表示</h3><p>Transformer中的输入向量由三部分组成:</p>\n<p>(1) 词嵌入(Token Embeddings):</p>\n<ul>\n<li>表示单词本身的语义</li>\n<li>通过可训练的嵌入矩阵获得</li>\n<li>维度通常是512维</li>\n</ul>\n<p>(2) 位置嵌入(Positional Embeddings):</p>\n<ul>\n<li>表示单词在序列中的位置信息</li>\n<li>使用正弦和余弦函数计算</li>\n<li>与词嵌入维度相同</li>\n</ul>\n<p>(3) 段嵌入(Segment Embeddings):</p>\n<ul>\n<li>用于区分不同的句子或段落</li>\n<li>主要用于BERT等模型</li>\n</ul>\n<h3 id=\"向量的计算过程\"><a href=\"#向量的计算过程\" class=\"headerlink\" title=\"向量的计算过程\"></a>向量的计算过程</h3><p>(1) 首先进行分词(Tokenization):</p>\n<ul>\n<li>将输入文本分解为token</li>\n<li>每个token映射到一个唯一的ID</li>\n<li>构建词表(Vocabulary)</li>\n</ul>\n<p>(2) 通过嵌入层获取词向量:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class=\"line\">token_embedding = embedding(token_ids)</span><br></pre></td></tr></table></figure>\n<p>(3) 计算位置编码:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PE(pos,<span class=\"number\">2</span>i) = sin(pos/<span class=\"number\">10000</span>^(<span class=\"number\">2</span>i/d))</span><br><span class=\"line\">PE(pos,<span class=\"number\">2</span>i+<span class=\"number\">1</span>) = cos(pos/<span class=\"number\">10000</span>^(<span class=\"number\">2</span>i/d))</span><br></pre></td></tr></table></figure>\n<p>(4) 将词向量和位置编码相加:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">final_embedding = token_embedding + positional_embedding</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量的特点\"><a href=\"#向量的特点\" class=\"headerlink\" title=\"向量的特点\"></a>向量的特点</h3><ul>\n<li>维度固定:通常是512维或768维</li>\n<li>稠密表示:每个维度都有值,不是one-hot编码</li>\n<li>可训练:通过反向传播不断优化</li>\n<li>可解释:向量间的距离和运算有语义含义</li>\n</ul>\n<p>这样,每个输入token最终都被转换为一个包含语义信息和位置信息的高维向量,作为Transformer的输入。这种向量表示使得模型能够:</p>\n<ul>\n<li>理解词与词之间的语义关系</li>\n<li>捕捉序列中的位置信息</li>\n<li>并行处理整个序列</li>\n<li>处理变长的输入</li>\n</ul>\n<p>这就是Transformer中向量表示的基本原理。通过这种方式,模型可以有效地理解和处理自然语言。</p>\n","more":"</p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><h3 id=\"注意力机制的通俗理解\"><a href=\"#注意力机制的通俗理解\" class=\"headerlink\" title=\"注意力机制的通俗理解\"></a>注意力机制的通俗理解</h3><p>让我们通过几个生活中的例子来理解:</p>\n<p>例子1: 阅读理解<br>假设有这样一段话:<br>“小明很喜欢打篮球。他每天都会去操场练习。”</p>\n<p>当我们读到”他”这个字的时候,大脑会自动去寻找这个”他”指的是谁。通过上下文,我们知道”他”指的是”小明”。这就是一个最基本的注意力机制 - 我们的大脑会自动关注到相关的重要信息。</p>\n<p>例子2: 歧义词理解<br>“我在得物上买了最新款的苹果,体验非常好”<br>“我在得物上买了阿克苏的苹果,口感非常好”</p>\n<p>在这两句话中,”苹果”是一个歧义词:</p>\n<ul>\n<li>第一句中的”苹果”指的是iPhone手机,因为搭配了”最新款”和”体验”</li>\n<li>第二句中的”苹果”指的是水果,因为搭配了”阿克苏”和”口感”</li>\n</ul>\n<p>注意力机制就是帮助模型像人类大脑一样,通过上下文来理解每个词的真实含义。</p>\n<h3 id=\"注意力机制是如何工作的\"><a href=\"#注意力机制是如何工作的\" class=\"headerlink\" title=\"注意力机制是如何工作的\"></a>注意力机制是如何工作的</h3><p>让我用一个形象的例子来解释:</p>\n<p>想象你在看一个很长的句子:”昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>当模型在处理”她”这个词时,会:</p>\n<p>1) 计算相关度分数:</p>\n<ul>\n<li>“她” 和 “小明” 的相关度: 较低(因为性别不匹配)</li>\n<li>“她” 和 “小红” 的相关度: 很高(因为性别匹配)</li>\n<li>“她” 和其他词的相关度: 较低</li>\n</ul>\n<p>2) 分配注意力权重:</p>\n<ul>\n<li>“小红” 获得最高的注意力权重</li>\n<li>其他词获得较低的注意力权重</li>\n</ul>\n<p>3) 整合信息:<br>模型会重点关注高权重的信息,从而理解”她”指代的是”小红”。</p>\n<h3 id=\"多头注意力机制\"><a href=\"#多头注意力机制\" class=\"headerlink\" title=\"多头注意力机制\"></a>多头注意力机制</h3><p>继续用刚才的例子:<br>“昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>多头注意力就像是从多个角度来理解这句话:</p>\n<ul>\n<li><p>第1个注意力头可能关注人物关系:<br>发现”她”和”小红”的关联</p>\n</li>\n<li><p>第2个注意力头可能关注时间信息:<br>关注”昨天下午”这个时间点</p>\n</li>\n<li><p>第3个注意力头可能关注地点信息:<br>关注”公园”这个场所</p>\n</li>\n<li><p>第4个注意力头可能关注动作信息:<br>关注”遇到”这个动作</p>\n</li>\n</ul>\n<p>这就像多个人从不同角度来理解同一句话,最后把这些理解综合起来,得到更全面的理解。</p>\n<h3 id=\"为什么需要注意力机制\"><a href=\"#为什么需要注意力机制\" class=\"headerlink\" title=\"为什么需要注意力机制\"></a>为什么需要注意力机制</h3><p>在注意力机制出现之前,模型处理长文本有这些问题:</p>\n<ul>\n<li><p>无法很好地处理长距离依赖<br>比如在很长的文章中,可能会”忘记”前面提到的重要信息</p>\n</li>\n<li><p>不能并行处理<br>必须一个词一个词按顺序处理,效率较低</p>\n</li>\n</ul>\n<p>注意力机制解决了这些问题:</p>\n<ul>\n<li>可以直接建立任意两个词之间的联系</li>\n<li>支持并行计算,大大提高了处理效率</li>\n</ul>\n<h2 id=\"为什么可以并行\"><a href=\"#为什么可以并行\" class=\"headerlink\" title=\"为什么可以并行\"></a>为什么可以并行</h2><p>对比传统模型(RNN)和Transformer来解释为什么Transformer支持并行计算。</p>\n<h3 id=\"传统RNN模型的处理方式\"><a href=\"#传统RNN模型的处理方式\" class=\"headerlink\" title=\"传统RNN模型的处理方式\"></a>传统RNN模型的处理方式</h3><p>假设我们有一句话：”我喜欢吃苹果”</p>\n<p>RNN必须按顺序一个词一个词处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">第1步：处理&quot;我&quot;</span><br><span class=\"line\">第2步：处理&quot;喜欢&quot; (必须等第1步完成)</span><br><span class=\"line\">第3步：处理&quot;吃&quot; (必须等第2步完成)</span><br><span class=\"line\">第4步：处理&quot;苹果&quot; (必须等第3步完成)</span><br></pre></td></tr></table></figure>\n<p>就像是在读一本书，你必须从第一页开始，按顺序一页一页读下去，不能跳着读。这就是”顺序处理”。</p>\n<h3 id=\"Transformer的并行处理方式\"><a href=\"#Transformer的并行处理方式\" class=\"headerlink\" title=\"Transformer的并行处理方式\"></a>Transformer的并行处理方式</h3><p>Transformer可以同时处理所有的词：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">同时处理：</span><br><span class=\"line\">- &quot;我&quot; </span><br><span class=\"line\">- &quot;喜欢&quot;</span><br><span class=\"line\">- &quot;吃&quot;</span><br><span class=\"line\">- &quot;苹果&quot;</span><br></pre></td></tr></table></figure>\n<p>这就像是有4个人，每个人负责读一个词，他们可以同时开始读。这就是”并行处理”。</p>\n<h3 id=\"为什么Transformer能做到并行？\"><a href=\"#为什么Transformer能做到并行？\" class=\"headerlink\" title=\"为什么Transformer能做到并行？\"></a>为什么Transformer能做到并行？</h3><p>关键在于位置编码(Positional Encoding)和自注意力机制：</p>\n<p>a) 位置编码：</p>\n<ul>\n<li>每个词都会被赋予一个位置信息</li>\n<li>比如：”我”(位置1)、”喜欢”(位置2)、”吃”(位置3)、”苹果”(位置4)</li>\n<li>这样即使同时处理所有词，模型也知道每个词在句子中的位置</li>\n</ul>\n<p>举个例子：<br>就像给每个人发一张带编号的扑克牌：</p>\n<ul>\n<li>“我” → 1号牌</li>\n<li>“喜欢” → 2号牌</li>\n<li>“吃” → 3号牌</li>\n<li>“苹果” → 4号牌</li>\n</ul>\n<p>这样即使大家同时看自己的牌，也知道这些词的正确顺序。</p>\n<p>b) 自注意力机制：</p>\n<ul>\n<li>每个词可以同时和句子中的所有其他词计算注意力分数</li>\n<li>不需要等待前面的词处理完成</li>\n</ul>\n<p>举个例子：<br>想象一个会议室里有4个人，每个人负责一个词：</p>\n<ul>\n<li>A负责”我”</li>\n<li>B负责”喜欢”</li>\n<li>C负责”吃”</li>\n<li>D负责”苹果”</li>\n</ul>\n<p>他们可以同时开始工作：</p>\n<ol>\n<li>每个人都有一份完整的句子副本</li>\n<li>他们可以同时查看整个句子</li>\n<li><p>各自计算自己负责的词与其他词的关联度</p>\n</li>\n<li><p>实际的计算优势：</p>\n</li>\n</ol>\n<p>假设处理一个有100个词的句子：</p>\n<ul>\n<li>RNN需要100个时间步骤，因为必须一个词一个词处理</li>\n<li>Transformer只需要1个时间步骤，因为可以同时处理100个词</li>\n</ul>\n<p>这就像是：</p>\n<ul>\n<li>RNN：一个人要读100页书，需要花100分钟</li>\n<li>Transformer：100个人同时读，每人读1页，只需要1分钟</li>\n</ul>\n<h3 id=\"在GPU上的优势\"><a href=\"#在GPU上的优势\" class=\"headerlink\" title=\"在GPU上的优势\"></a>在GPU上的优势</h3><p>现代GPU擅长并行计算，就像有很多个小工人可以同时工作。</p>\n<ul>\n<li>Transformer充分利用了这一优势，可以让GPU同时处理多个词</li>\n<li>而RNN因为必须按顺序处理，无法充分利用GPU的并行能力</li>\n</ul>\n<p>这就是为什么说Transformer支持并行计算，它通过巧妙的设计（位置编码+自注意力机制），让所有词可以同时被处理，大大提高了计算效率。这也是Transformer相比RNN的一个重要优势。</p>\n<h2 id=\"注意力机制计算过程\"><a href=\"#注意力机制计算过程\" class=\"headerlink\" title=\"注意力机制计算过程\"></a>注意力机制计算过程</h2><h3 id=\"基本概念：Query-查询-、Key-键-、Value-值\"><a href=\"#基本概念：Query-查询-、Key-键-、Value-值\" class=\"headerlink\" title=\"基本概念：Query(查询)、Key(键)、Value(值)\"></a>基本概念：Query(查询)、Key(键)、Value(值)</h3><p>先用一个生活中的例子来理解这三个概念：<br>想象你去图书馆找书：</p>\n<ul>\n<li>Query(查询)：你想找的书的关键词，比如”人工智能入门”</li>\n<li>Key(键)：图书馆里每本书的标题</li>\n<li>Value(值)：书的具体内容</li>\n</ul>\n<h3 id=\"在Transformer中的对应关系\"><a href=\"#在Transformer中的对应关系\" class=\"headerlink\" title=\"在Transformer中的对应关系\"></a>在Transformer中的对应关系</h3><p>还是用前面的例子：<br>“昨天下午,小明在公园里遇到了小红,她穿着一条蓝色的裙子。”</p>\n<p>当我们要理解”她”这个词时：</p>\n<ul>\n<li>Query(查询)：是”她”这个词的表示向量</li>\n<li>Key(键)：句子中所有词的表示向量</li>\n<li>Value(值)：句子中所有词携带的信息</li>\n</ul>\n<h3 id=\"具体计算步骤\"><a href=\"#具体计算步骤\" class=\"headerlink\" title=\"具体计算步骤\"></a>具体计算步骤</h3><p>让我们用一个简化的例子来说明：<br>“小红很喜欢吃苹果”</p>\n<p>第一步：转换为向量<br>假设我们用2维向量来表示每个词（实际中通常是512维或更高）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;小红&quot; → [2, 3]</span><br><span class=\"line\">&quot;很&quot; → [1, 1]</span><br><span class=\"line\">&quot;喜欢&quot; → [2, 2]</span><br><span class=\"line\">&quot;吃&quot; → [1, 2]</span><br><span class=\"line\">&quot;苹果&quot; → [3, 1]</span><br></pre></td></tr></table></figure>\n<p>第二步：计算Query、Key、Value<br>每个词都会被转换为三种向量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">例如&quot;小红&quot;：</span><br><span class=\"line\">Q向量(查询)：[2, 3] × WQ = [4, 6]</span><br><span class=\"line\">K向量(键)：[2, 3] × WK = [5, 7]</span><br><span class=\"line\">V向量(值)：[2, 3] × WV = [3, 4]</span><br></pre></td></tr></table></figure>\n<p>(其中WQ、WK、WV是可学习的权重矩阵)</p>\n<p>第三步：计算注意力分数<br>以”小红”为例，计算它与所有词的相关度：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">相关度 = Q向量 · K向量 / √维度</span><br><span class=\"line\"></span><br><span class=\"line\">比如&quot;小红&quot;和&quot;苹果&quot;的相关度：</span><br><span class=\"line\">= [4, 6] · [5, 7] / √2</span><br><span class=\"line\">= (4×5 + 6×7) / 1.414</span><br><span class=\"line\">= (20 + 42) / 1.414</span><br><span class=\"line\">= 43.7</span><br></pre></td></tr></table></figure>\n<p>第四步：归一化分数<br>使用softmax函数将所有分数转换为0-1之间的概率值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原始分数：[43.7, 35.2, 28.9, 31.5, 39.6]</span><br><span class=\"line\">归一化后：[0.3, 0.2, 0.1, 0.15, 0.25]</span><br></pre></td></tr></table></figure>\n<p>第五步：加权求和<br>用这些概率值去加权每个词的Value向量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">最终表示 = 0.3×V1 + 0.2×V2 + 0.1×V3 + 0.15×V4 + 0.25×V5</span><br></pre></td></tr></table></figure>\n<h3 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h3><p>实际上，Transformer使用了多头注意力机制，相当于：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- 注意力头1：可能关注语法关系</span><br><span class=\"line\">- 注意力头2：可能关注语义关系</span><br><span class=\"line\">- 注意力头3：可能关注位置关系</span><br><span class=\"line\">...（通常有8个头）</span><br></pre></td></tr></table></figure>\n<p>每个头都独立计算注意力分数，最后将所有结果合并。</p>\n<h3 id=\"形象的比喻\"><a href=\"#形象的比喻\" class=\"headerlink\" title=\"形象的比喻\"></a>形象的比喻</h3><p>这个过程就像：</p>\n<ul>\n<li>一个人(Query)在派对上(句子)寻找聊天对象</li>\n<li>他会看看每个人(Key)是否适合聊天</li>\n<li>计算相关度就像评估与每个人的共同话题多少</li>\n<li>最后主要和相关度高的人(Value)交流</li>\n</ul>\n<p>或者像：</p>\n<ul>\n<li>一个学生(Query)在解答问题</li>\n<li>他会查看所有笔记(Key)</li>\n<li>计算每页笔记与问题的相关度</li>\n<li>最后主要参考相关度高的笔记内容(Value)</li>\n</ul>\n<p>这就是注意力分数的计算过程。虽然数学计算看起来复杂，但基本思想是让模型能够像人类一样，通过计算相关度来决定应该重点关注哪些信息。</p>\n<h2 id=\"向量\"><a href=\"#向量\" class=\"headerlink\" title=\"向量\"></a>向量</h2><h3 id=\"词向量的基本概念\"><a href=\"#词向量的基本概念\" class=\"headerlink\" title=\"词向量的基本概念\"></a>词向量的基本概念</h3><p>词向量(Word Embedding)是将单词转换为计算机可以理解的数值向量的方法。主要有以下特点：</p>\n<ul>\n<li>每个单词被映射到一个固定维度(比如300维)的向量空间中</li>\n<li>语义相近的词在向量空间中的距离也相近</li>\n<li>向量之间可以进行数学运算,比如:<br>vector(‘国王’) - vector(‘男人’) + vector(‘女人’) ≈ vector(‘女王’)</li>\n</ul>\n<h3 id=\"获取词向量的主要方法\"><a href=\"#获取词向量的主要方法\" class=\"headerlink\" title=\"获取词向量的主要方法\"></a>获取词向量的主要方法</h3><p>(1) Word2Vec模型:</p>\n<ul>\n<li>CBOW(Continuous Bag of Words):使用上下文预测目标词</li>\n<li>Skip-gram:使用目标词预测上下文词</li>\n<li>通过神经网络训练,学习词与词之间的语义关系</li>\n</ul>\n<p>(2) GloVe模型:</p>\n<ul>\n<li>基于全局矩阵分解</li>\n<li>统计词与词的共现频率</li>\n<li>结合局部上下文窗口方法</li>\n</ul>\n<p>(3) FastText模型:</p>\n<ul>\n<li>将单词分解为子词单元(n-gram)</li>\n<li>能处理词表外的词</li>\n<li>对拼写错误有一定容忍度</li>\n</ul>\n<h3 id=\"Transformer中的向量表示\"><a href=\"#Transformer中的向量表示\" class=\"headerlink\" title=\"Transformer中的向量表示\"></a>Transformer中的向量表示</h3><p>Transformer中的输入向量由三部分组成:</p>\n<p>(1) 词嵌入(Token Embeddings):</p>\n<ul>\n<li>表示单词本身的语义</li>\n<li>通过可训练的嵌入矩阵获得</li>\n<li>维度通常是512维</li>\n</ul>\n<p>(2) 位置嵌入(Positional Embeddings):</p>\n<ul>\n<li>表示单词在序列中的位置信息</li>\n<li>使用正弦和余弦函数计算</li>\n<li>与词嵌入维度相同</li>\n</ul>\n<p>(3) 段嵌入(Segment Embeddings):</p>\n<ul>\n<li>用于区分不同的句子或段落</li>\n<li>主要用于BERT等模型</li>\n</ul>\n<h3 id=\"向量的计算过程\"><a href=\"#向量的计算过程\" class=\"headerlink\" title=\"向量的计算过程\"></a>向量的计算过程</h3><p>(1) 首先进行分词(Tokenization):</p>\n<ul>\n<li>将输入文本分解为token</li>\n<li>每个token映射到一个唯一的ID</li>\n<li>构建词表(Vocabulary)</li>\n</ul>\n<p>(2) 通过嵌入层获取词向量:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class=\"line\">token_embedding = embedding(token_ids)</span><br></pre></td></tr></table></figure>\n<p>(3) 计算位置编码:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PE(pos,<span class=\"number\">2</span>i) = sin(pos/<span class=\"number\">10000</span>^(<span class=\"number\">2</span>i/d))</span><br><span class=\"line\">PE(pos,<span class=\"number\">2</span>i+<span class=\"number\">1</span>) = cos(pos/<span class=\"number\">10000</span>^(<span class=\"number\">2</span>i/d))</span><br></pre></td></tr></table></figure>\n<p>(4) 将词向量和位置编码相加:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">final_embedding = token_embedding + positional_embedding</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量的特点\"><a href=\"#向量的特点\" class=\"headerlink\" title=\"向量的特点\"></a>向量的特点</h3><ul>\n<li>维度固定:通常是512维或768维</li>\n<li>稠密表示:每个维度都有值,不是one-hot编码</li>\n<li>可训练:通过反向传播不断优化</li>\n<li>可解释:向量间的距离和运算有语义含义</li>\n</ul>\n<p>这样,每个输入token最终都被转换为一个包含语义信息和位置信息的高维向量,作为Transformer的输入。这种向量表示使得模型能够:</p>\n<ul>\n<li>理解词与词之间的语义关系</li>\n<li>捕捉序列中的位置信息</li>\n<li>并行处理整个序列</li>\n<li>处理变长的输入</li>\n</ul>\n<p>这就是Transformer中向量表示的基本原理。通过这种方式,模型可以有效地理解和处理自然语言。</p>","categories":[],"tags":[{"name":"AI","path":"api/tags/AI.json"}]}