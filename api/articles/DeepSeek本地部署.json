{"title":"DeepSeek本地部署相关","slug":"DeepSeek本地部署","date":"2025-04-26T02:18:36.000Z","updated":"2025-04-26T02:18:36.000Z","comments":true,"path":"api/articles/DeepSeek本地部署.json","excerpt":"本地部署DeepSeek的简要介绍<br>","covers":["../images/deepseek_1.png"],"content":"<p>本地部署DeepSeek的简要介绍<br><a id=\"more\"></a></p>\n<h2 id=\"本地部署优点\"><a href=\"#本地部署优点\" class=\"headerlink\" title=\"本地部署优点\"></a>本地部署优点</h2><ul>\n<li>免费</li>\n<li>数据隐私</li>\n<li>无额外限制（可绕过敏感话题，纯粹基于数据和算法逻辑）</li>\n<li>无需网络依赖</li>\n<li>灵活定制（根据自己的知识库配置微调）</li>\n<li>性能和效率（不会卡顿延迟服务器繁忙）</li>\n</ul>\n<h2 id=\"本地部署缺点\"><a href=\"#本地部署缺点\" class=\"headerlink\" title=\"本地部署缺点\"></a>本地部署缺点</h2><p>本地部署有一个比较大的局限性，很吃设备的配置</p>\n<ul>\n<li>满血版：DeepSeek 的完整版本，通常具有非常大的参数量。eg：DeepSeek-R1</li>\n<li>蒸馏版本：通过知识蒸馏技术从满血版模型中提取关键知识并转移到更小的模型中，从而在保持较高性能的同时，显著降低计算资源需求。<br>DeepSeek-R1-Distill-Qwen-32B（Distill代表蒸馏，Qwen代表是基于阿里的开源大模型千问，32B代表模型中可训练参数的数量）</li>\n</ul>\n<p>从性价比角度，32b就已经很不错了，当然满血版更好。<br><img src=\"../images/deepseek_1.png\" alt=\"deepseek_1\"></p>\n<h2 id=\"🔗-官方文档位置\"><a href=\"#🔗-官方文档位置\" class=\"headerlink\" title=\"🔗 官方文档位置\"></a>🔗 官方文档位置</h2><p><strong>主要来源</strong>：<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\" rel=\"noopener\"><strong>DeepSeek-R1 GitHub 仓库</strong></a></p>\n<h2 id=\"📋-系统要求\"><a href=\"#📋-系统要求\" class=\"headerlink\" title=\"📋 系统要求\"></a>📋 系统要求</h2><h3 id=\"硬件要求\"><a href=\"#硬件要求\" class=\"headerlink\" title=\"硬件要求\"></a>硬件要求</h3><table>\n<thead>\n<tr>\n<th>模型大小</th>\n<th>GPU 显存</th>\n<th>系统内存</th>\n<th>存储空间</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1.5B-7B</td>\n<td>8GB+</td>\n<td>16GB+</td>\n<td>50GB+</td>\n</tr>\n<tr>\n<td>8B-14B</td>\n<td>16GB+</td>\n<td>32GB+</td>\n<td>100GB+</td>\n</tr>\n<tr>\n<td>32B+</td>\n<td>32GB+</td>\n<td>64GB+</td>\n<td>200GB+</td>\n</tr>\n<tr>\n<td>67B+</td>\n<td>80GB+</td>\n<td>128GB+</td>\n<td>500GB+</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"软件环境\"><a href=\"#软件环境\" class=\"headerlink\" title=\"软件环境\"></a>软件环境</h3><ul>\n<li><strong>操作系统</strong>：Linux/macOS/Windows</li>\n<li><strong>Python</strong>：3.8+</li>\n<li><strong>CUDA</strong>：11.8+ (NVIDIA GPU)</li>\n<li><strong>工具</strong>：Ollama/vLLM/Transformers</li>\n</ul>\n<h2 id=\"🚀-三种主要安装方法\"><a href=\"#🚀-三种主要安装方法\" class=\"headerlink\" title=\"🚀 三种主要安装方法\"></a>🚀 三种主要安装方法</h2><h3 id=\"方法-1：使用-Ollama（推荐新手）\"><a href=\"#方法-1：使用-Ollama（推荐新手）\" class=\"headerlink\" title=\"方法 1：使用 Ollama（推荐新手）\"></a>方法 1：使用 Ollama（推荐新手）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 安装Ollama</span></span><br><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 拉取模型</span></span><br><span class=\"line\">ollama pull deepseek-r1:7b</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 运行模型</span></span><br><span class=\"line\">ollama run deepseek-r1:7b</span><br></pre></td></tr></table></figure>\n<h3 id=\"方法-2：使用-vLLM（生产环境）\"><a href=\"#方法-2：使用-vLLM（生产环境）\" class=\"headerlink\" title=\"方法 2：使用 vLLM（生产环境）\"></a>方法 2：使用 vLLM（生产环境）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 安装依赖</span></span><br><span class=\"line\">pip install vllm transformers</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 启动服务</span></span><br><span class=\"line\">vllm serve <span class=\"string\">\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"</span> --max_model 4096</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. API调用</span></span><br><span class=\"line\">curl -X POST <span class=\"string\">\"http://localhost:8000/v1/chat/completions\"</span> \\</span><br><span class=\"line\">    -H <span class=\"string\">\"Content-Type: application/json\"</span> \\</span><br><span class=\"line\">    --data <span class=\"string\">'&#123;\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"messages\": [&#123;\"role\": \"user\", \"content\": \"你好\"&#125;]&#125;'</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"方法-3：使用-Transformers（研究用途）\"><a href=\"#方法-3：使用-Transformers（研究用途）\" class=\"headerlink\" title=\"方法 3：使用 Transformers（研究用途）\"></a>方法 3：使用 Transformers（研究用途）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> pipeline</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建管道</span></span><br><span class=\"line\">pipe = pipeline(<span class=\"string\">\"text-generation\"</span>, model=<span class=\"string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用模型</span></span><br><span class=\"line\">messages = [&#123;<span class=\"string\">\"role\"</span>: <span class=\"string\">\"user\"</span>, <span class=\"string\">\"content\"</span>: <span class=\"string\">\"你好，请介绍一下自己\"</span>&#125;]</span><br><span class=\"line\">response = pipe(messages)</span><br><span class=\"line\">print(response)</span><br></pre></td></tr></table></figure>\n<h2 id=\"📚-官方资源链接\"><a href=\"#📚-官方资源链接\" class=\"headerlink\" title=\"📚 官方资源链接\"></a>📚 <strong>官方资源链接</strong></h2><ol>\n<li><strong><a href=\"https://github.com/deepseek-ai/DeepSeek-R1\" target=\"_blank\" rel=\"noopener\">GitHub 仓库</a></strong> - 90k⭐，包含完整代码和文档</li>\n<li><strong><a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\" rel=\"noopener\">技术论文</a></strong> - 详细技术细节</li>\n<li><strong><a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" rel=\"noopener\">Hugging Face 模型页面</a></strong> - 多个模型变体</li>\n<li><strong>DeepSeek 官网</strong> (deepseek.com) - 最新公告和指南</li>\n</ol>\n<h2 id=\"💡-快速开始建议\"><a href=\"#💡-快速开始建议\" class=\"headerlink\" title=\"💡 快速开始建议\"></a>💡 快速开始建议</h2><ol>\n<li><strong>检查硬件</strong>：确认 GPU 内存足够</li>\n<li><strong>选择模型大小</strong>：从 7B 模型开始测试</li>\n<li><strong>使用 Ollama</strong>：最简单的入门方式</li>\n<li><strong>逐步优化</strong>：根据需求调整配置</li>\n</ol>\n","more":"</p>\n<h2 id=\"本地部署优点\"><a href=\"#本地部署优点\" class=\"headerlink\" title=\"本地部署优点\"></a>本地部署优点</h2><ul>\n<li>免费</li>\n<li>数据隐私</li>\n<li>无额外限制（可绕过敏感话题，纯粹基于数据和算法逻辑）</li>\n<li>无需网络依赖</li>\n<li>灵活定制（根据自己的知识库配置微调）</li>\n<li>性能和效率（不会卡顿延迟服务器繁忙）</li>\n</ul>\n<h2 id=\"本地部署缺点\"><a href=\"#本地部署缺点\" class=\"headerlink\" title=\"本地部署缺点\"></a>本地部署缺点</h2><p>本地部署有一个比较大的局限性，很吃设备的配置</p>\n<ul>\n<li>满血版：DeepSeek 的完整版本，通常具有非常大的参数量。eg：DeepSeek-R1</li>\n<li>蒸馏版本：通过知识蒸馏技术从满血版模型中提取关键知识并转移到更小的模型中，从而在保持较高性能的同时，显著降低计算资源需求。<br>DeepSeek-R1-Distill-Qwen-32B（Distill代表蒸馏，Qwen代表是基于阿里的开源大模型千问，32B代表模型中可训练参数的数量）</li>\n</ul>\n<p>从性价比角度，32b就已经很不错了，当然满血版更好。<br><img src=\"../images/deepseek_1.png\" alt=\"deepseek_1\"></p>\n<h2 id=\"🔗-官方文档位置\"><a href=\"#🔗-官方文档位置\" class=\"headerlink\" title=\"🔗 官方文档位置\"></a>🔗 官方文档位置</h2><p><strong>主要来源</strong>：<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\" rel=\"noopener\"><strong>DeepSeek-R1 GitHub 仓库</strong></a></p>\n<h2 id=\"📋-系统要求\"><a href=\"#📋-系统要求\" class=\"headerlink\" title=\"📋 系统要求\"></a>📋 系统要求</h2><h3 id=\"硬件要求\"><a href=\"#硬件要求\" class=\"headerlink\" title=\"硬件要求\"></a>硬件要求</h3><table>\n<thead>\n<tr>\n<th>模型大小</th>\n<th>GPU 显存</th>\n<th>系统内存</th>\n<th>存储空间</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1.5B-7B</td>\n<td>8GB+</td>\n<td>16GB+</td>\n<td>50GB+</td>\n</tr>\n<tr>\n<td>8B-14B</td>\n<td>16GB+</td>\n<td>32GB+</td>\n<td>100GB+</td>\n</tr>\n<tr>\n<td>32B+</td>\n<td>32GB+</td>\n<td>64GB+</td>\n<td>200GB+</td>\n</tr>\n<tr>\n<td>67B+</td>\n<td>80GB+</td>\n<td>128GB+</td>\n<td>500GB+</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"软件环境\"><a href=\"#软件环境\" class=\"headerlink\" title=\"软件环境\"></a>软件环境</h3><ul>\n<li><strong>操作系统</strong>：Linux/macOS/Windows</li>\n<li><strong>Python</strong>：3.8+</li>\n<li><strong>CUDA</strong>：11.8+ (NVIDIA GPU)</li>\n<li><strong>工具</strong>：Ollama/vLLM/Transformers</li>\n</ul>\n<h2 id=\"🚀-三种主要安装方法\"><a href=\"#🚀-三种主要安装方法\" class=\"headerlink\" title=\"🚀 三种主要安装方法\"></a>🚀 三种主要安装方法</h2><h3 id=\"方法-1：使用-Ollama（推荐新手）\"><a href=\"#方法-1：使用-Ollama（推荐新手）\" class=\"headerlink\" title=\"方法 1：使用 Ollama（推荐新手）\"></a>方法 1：使用 Ollama（推荐新手）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 安装Ollama</span></span><br><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 拉取模型</span></span><br><span class=\"line\">ollama pull deepseek-r1:7b</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 运行模型</span></span><br><span class=\"line\">ollama run deepseek-r1:7b</span><br></pre></td></tr></table></figure>\n<h3 id=\"方法-2：使用-vLLM（生产环境）\"><a href=\"#方法-2：使用-vLLM（生产环境）\" class=\"headerlink\" title=\"方法 2：使用 vLLM（生产环境）\"></a>方法 2：使用 vLLM（生产环境）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 安装依赖</span></span><br><span class=\"line\">pip install vllm transformers</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 启动服务</span></span><br><span class=\"line\">vllm serve <span class=\"string\">\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"</span> --max_model 4096</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. API调用</span></span><br><span class=\"line\">curl -X POST <span class=\"string\">\"http://localhost:8000/v1/chat/completions\"</span> \\</span><br><span class=\"line\">    -H <span class=\"string\">\"Content-Type: application/json\"</span> \\</span><br><span class=\"line\">    --data <span class=\"string\">'&#123;\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"messages\": [&#123;\"role\": \"user\", \"content\": \"你好\"&#125;]&#125;'</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"方法-3：使用-Transformers（研究用途）\"><a href=\"#方法-3：使用-Transformers（研究用途）\" class=\"headerlink\" title=\"方法 3：使用 Transformers（研究用途）\"></a>方法 3：使用 Transformers（研究用途）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> pipeline</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建管道</span></span><br><span class=\"line\">pipe = pipeline(<span class=\"string\">\"text-generation\"</span>, model=<span class=\"string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用模型</span></span><br><span class=\"line\">messages = [&#123;<span class=\"string\">\"role\"</span>: <span class=\"string\">\"user\"</span>, <span class=\"string\">\"content\"</span>: <span class=\"string\">\"你好，请介绍一下自己\"</span>&#125;]</span><br><span class=\"line\">response = pipe(messages)</span><br><span class=\"line\">print(response)</span><br></pre></td></tr></table></figure>\n<h2 id=\"📚-官方资源链接\"><a href=\"#📚-官方资源链接\" class=\"headerlink\" title=\"📚 官方资源链接\"></a>📚 <strong>官方资源链接</strong></h2><ol>\n<li><strong><a href=\"https://github.com/deepseek-ai/DeepSeek-R1\" target=\"_blank\" rel=\"noopener\">GitHub 仓库</a></strong> - 90k⭐，包含完整代码和文档</li>\n<li><strong><a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\" rel=\"noopener\">技术论文</a></strong> - 详细技术细节</li>\n<li><strong><a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" rel=\"noopener\">Hugging Face 模型页面</a></strong> - 多个模型变体</li>\n<li><strong>DeepSeek 官网</strong> (deepseek.com) - 最新公告和指南</li>\n</ol>\n<h2 id=\"💡-快速开始建议\"><a href=\"#💡-快速开始建议\" class=\"headerlink\" title=\"💡 快速开始建议\"></a>💡 快速开始建议</h2><ol>\n<li><strong>检查硬件</strong>：确认 GPU 内存足够</li>\n<li><strong>选择模型大小</strong>：从 7B 模型开始测试</li>\n<li><strong>使用 Ollama</strong>：最简单的入门方式</li>\n<li><strong>逐步优化</strong>：根据需求调整配置</li>\n</ol>","categories":[],"tags":[{"name":"AI","path":"api/tags/AI.json"}]}